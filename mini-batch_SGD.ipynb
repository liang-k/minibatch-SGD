{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters(w,b)\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    :param layer_dims: list,每一层单元的个数（维度）\n",
    "    :return:dictionary,存储参数w1,w2,...,wL,b1,...,bL\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    L = len(layer_dims)  # the number of layers in the network\n",
    "    parameters = {}\n",
    "    for l in range(1, L):\n",
    "        # parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(\n",
    "            2 / layer_dims[l - 1])  # he initialization\n",
    "        # parameters[\"W\" + str(l)] = np.zeros((layer_dims[l], layer_dims[l - 1])) #为了测试初始化为0的后果\n",
    "        # parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(1 / layer_dims[l - 1])  # xavier initialization\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    :param Z: Output of the linear layer\n",
    "    :return:\n",
    "    A: output of activation\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    return A\n",
    "\n",
    "\n",
    "# implement the activation function(ReLU and sigmoid)\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    :param Z: Output of the linear layer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\",...,\"WL\", \"bL\"\n",
    "                    W -- weight matrix of shape (size of current layer, size of previous layer)\n",
    "                    b -- bias vector of shape (size of current layer,1)\n",
    "    :return:\n",
    "    AL: the output of the last Layer(y_predict)\n",
    "    caches: list, every element is a tuple:(W,b,z,A_pre)\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layer\n",
    "    A = X\n",
    "    caches = [(None, None, None, X)]  # 第0层(None,None,None,A0) w,b,z用none填充,下标与层数一致，用于存储每一层的，w,b,z,A\n",
    "    # calculate from 1 to L-1 layer\n",
    "    for l in range(1, L):\n",
    "        A_pre = A\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        z = np.dot(W, A_pre) + b  # 计算z = wx + b\n",
    "        A = relu(z)  # relu activation function\n",
    "        caches.append((W, b, z, A))\n",
    "    # calculate Lth layer\n",
    "    WL = parameters[\"W\" + str(L)]\n",
    "    bL = parameters[\"b\" + str(L)]\n",
    "    zL = np.dot(WL, A) + bL\n",
    "    AL = sigmoid(zL)\n",
    "    caches.append((WL, bL, zL, AL))\n",
    "    return AL, caches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cost function\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    :param AL: 最后一层的激活值，即预测值，shape:(1,number of examples)\n",
    "    :param Y:真实值,shape:(1, number of examples)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    # cost = -1.0/m * np.sum(Y*np.log(AL)+(1-Y)*np.log(1.0 - AL))#py中*是点乘\n",
    "    # cost = (1. / m) * (-np.dot(Y, np.log(AL).T) - np.dot(1 - Y, np.log(1 - AL).T)) #推荐用这个，上面那个容易出错\n",
    "    cost = 1. / m * np.nansum(np.multiply(-np.log(AL), Y) +\n",
    "                              np.multiply(-np.log(1 - AL), 1 - Y))\n",
    "    # 从数组的形状中删除单维条目，即把shape中为1的维度去掉，比如把[[[2]]]变成2\n",
    "    cost = np.squeeze(cost)\n",
    "    # print('=====================cost===================')\n",
    "    # print(cost)\n",
    "    return cost\n",
    "\n",
    "\n",
    "# derivation of relu\n",
    "def relu_backward(Z):\n",
    "    \"\"\"\n",
    "    :param Z: the input of activation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dA = np.int64(Z > 0)\n",
    "    return dA\n",
    "\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    caches -- caches output from forward_propagation(),(W,b,z,pre_A)\n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to dW,db\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    L = len(caches) - 1\n",
    "    # print(\"L:   \" + str(L))\n",
    "    # calculate the Lth layer gradients\n",
    "    prev_AL = caches[L - 1][3]\n",
    "    dzL = 1. / m * (AL - Y)\n",
    "    # print(dzL.shape)\n",
    "    # print(prev_AL.T.shape)\n",
    "    dWL = np.dot(dzL, prev_AL.T)\n",
    "    dbL = np.sum(dzL, axis=1, keepdims=True)\n",
    "    gradients = {\"dW\" + str(L): dWL, \"db\" + str(L): dbL}\n",
    "    # calculate from L-1 to 1 layer gradients\n",
    "    for l in reversed(range(1, L)):  # L-1,L-3,....,1\n",
    "        post_W = caches[l + 1][0]  # 要用后一层的W\n",
    "        dz = dzL  # 用后一层的dz\n",
    "\n",
    "        dal = np.dot(post_W.T, dz)\n",
    "        z = caches[l][2]  # 当前层的z\n",
    "        dzl = np.multiply(dal, relu_backward(z))\n",
    "        prev_A = caches[l - 1][3]  # 前一层的A\n",
    "        dWl = np.dot(dzl, prev_A.T)\n",
    "        dbl = np.sum(dzl, axis=1, keepdims=True)\n",
    "\n",
    "        gradients[\"dW\" + str(l)] = dWl\n",
    "        gradients[\"db\" + str(l)] = dbl\n",
    "        dzL = dzl  # 更新dz\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    :param parameters: dictionary,  W,b\n",
    "    :param grads: dW,db\n",
    "    :param learning_rate: alpha\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=1):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = m // mini_batch_size  # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size: (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def L_layer_model(X, Y, layer_dims, learning_rate, num_iterations, gradient_descent='mini-batch', mini_batch_size=64):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param Y:\n",
    "    :param layer_dims:list containing the input size and each layer size\n",
    "    :param learning_rate:\n",
    "    :param num_iterations:\n",
    "    :return:\n",
    "    parameters：final parameters:(W,b)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    costs = []\n",
    "    # initialize parameters\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "    gradient_descent == 'mini-batch'\n",
    "    seed = 0\n",
    "    for i in range(0, num_iterations):\n",
    "    # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # Forward propagation\n",
    "                AL, caches = forward_propagation(minibatch_X, parameters)\n",
    "                # Compute cost\n",
    "                cost = compute_cost(AL, minibatch_Y)\n",
    "                # Backward propagation\n",
    "                grads = backward_propagation(AL, minibatch_Y, caches)\n",
    "                parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                if i % 100 == 0:\n",
    "                     print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "                costs.append(cost)\n",
    "    print('length of cost')\n",
    "    print(len(costs))\n",
    "    plt.clf()\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel(\"iterations(hundred)\")  # 横坐标名字\n",
    "    plt.ylabel(\"cost\")  # 纵坐标名字\n",
    "    plt.show()\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# predict function\n",
    "def predict(X_test, y_test, parameters):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param parameters:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = y_test.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    prob, caches = forward_propagation(X_test, parameters)\n",
    "    for i in range(prob.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if prob[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "    accuracy = 1 - np.mean(np.abs(Y_prediction - y_test))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN model\n",
    "def DNN(X_train, y_train, X_test, y_test, layer_dims, learning_rate=0.0006, num_iterations=30000,\n",
    "        gradient_descent='mini-batch', mini_batch_size=64):\n",
    "    parameters = L_layer_model(X_train, y_train, layer_dims, learning_rate, num_iterations, gradient_descent,\n",
    "                               mini_batch_size)\n",
    "    accuracy = predict(X_test, y_test, parameters)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_data, y_data = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, train_size=0.8, random_state=28)\n",
    "    X_train = X_train.T\n",
    "    y_train = y_train.reshape(y_train.shape[0], -1).T\n",
    "    X_test = X_test.T\n",
    "    y_test = y_test.reshape(y_test.shape[0], -1).T\n",
    "    # mini-batch\n",
    "    accuracy = DNN(X_train, y_train, X_test, y_test, [X_train.shape[0], 10, 5, 1], num_iterations=10000,\n",
    "                   gradient_descent='mini-batch')\n",
    "    print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
